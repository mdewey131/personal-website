---
author: "Michael Dewey"
slug: "making_transition_probs"
date: "2020-11-13"
output: blogdown::html_page
tags:
  - Rstats
  - data cleaning
categories:
  - projects
  - Snappa scoreboard
thumbnailImage: "/img/transition_probabilities.JPG"
thumbnailImagePosition: left
coverImage: "/img/transition_probabilities.JPG"
coverCaption: "A portion of the score transition probabilities on offense for myself and my most common Snappa partner. The upper triangular structure comes from the fact that you cannot score negative points."
coverMeta: out
metaAlignment: center
title: "Building Transition Probabilities From Scratch"
---
My last [article](/2020/11/snappa-markov-creation/) gave a high-level overview of the reasoning and 
methodology behind the Markov chain model which the Snappa Scoreboard uses
to give win probability estimates to users.


That article was intended to strike 
a balance between the formal mathematics behind the model and the intuitive reasoning
behind the modeling decision, and as a result it was rather light on the R code
used to obtain each of the objects that I use. 

Today, I thought it would be good to provide more of that technical description,
both as an attempt at something like [rubber duck debugging](https://en.wikipedia.org/wiki/Rubber_duck_debugging),
and as a guide to anybody who might need to obtain transition probabilities from a time 
series manually, particularly when trying to subdivide your clean data into different
matrices as I have done with this project. 

Since I'm explaining a process of data cleaning, I should first provide a primer
on the raw structure of data to fully explain how cleaning was done. 

## Existing Data Structure

```{r Import/Rename Data (hidden), include= F, eval = T}
library(DBI)
library(tidyverse)
library(gt)
source('../../../../OneDrive/Documents/Snappa-Scoreboard/dbconnect.R')
players = dbReadTable(con, "players")
names(players) = c("serial_num", "player_id")
performance = dbReadTable(con, "player_stats")
names(performance) = c("game_number", "serial_number", "side", 
                              "rounds", "point_total", "single_points", 
                              "double_points", "sink_points", "four_plus_points", 
                              "defensive_return_points", "off_glass_points", "production_per_round", 
                              "offensive_round_production", "defensive_round_production", 
                              "batting_average")

summary = dbReadTable(con, "game_stats")
names(summary) = c("game_number", "snappaneers", "start_time", 
                        "end_time", "night_game", "total_a", "total_b",
                        "total_rounds", "game_single_points", "game_double_points",
                        "game_sinks", "game_four_plus", "game_defensive_return",
                        "game_off_glass", "finished_game", "final_shot", "location" 
                        )
events = dbReadTable(con, "scores")
names(events) = c("shot_number", "game_number", "serial_num", "scored",
                          "round", "total", "offense", "defensive", "off_glass",
                          "kicked_back")
rm(con)
```

All Snappa data is stored in a database that updates to the cloud as a game is 
being played. There are 3 main tables which are of importance here^[Note: although
I don't believe it is a pressing issue for our small little application, I will not
be using the actual table names or their column names from our database for 
[Robert](https://xkcd.com/327/) reasons. Paranoia is not usually a good thing, 
but I dare not tempt the internet when it comes to matters of security]:

* A table of `events` which records the streaming game data, and is the 
basis for the other tables.
* A table on `performance`, which records player-level data for each game.
* A `summary` table, which rolls aggregates scores data to the game level. 

Let's look at a portion of the tables for one game to understand how they come together.
```{r Tables GT (shown), echo = F}
events %>% 
  filter(game_number ==  242) %>% 
  head(10) %>%
  gt() %>%
  tab_header(title = "Events",
             subtitle = "First 10 scoring events of game 242")

performance %>%
  filter(game_number == 242) %>%
  gt() %>%
  tab_header(title = "Performance",
             subtitle = "A complete table entry for game 242")

summary %>%
  filter(game_number == 242) %>%
  select(-c(start_time, end_time)) %>%
  mutate(location = "Snappa Home Base") %>%
  gt() %>%
  tab_header(title = "Summary",
             subtitle = "The single row for game 242 in the table")


```

As you can see, `events` records everything that we can, given the user 
input, about the score which has just been recorded. The rows of the table are
grouped by the `serial_number` of the player, and it is those sums and shot
percentages which make the data for `performance`. All of the players information
is summed up in `summary`, and everything is linked together by the game's
`game_number`. You may also notice that not *everything* is an aggregation of 
`scoring_events`. There is some additional information added to `summary`,
for example the game's `location`, which does not come from `events`. Each table
expresses a slightly different level of aggregation, and so there are naturally some 
variables which are game-level attributes and therefore do not make their way into
`events`. 

## Specifying our Target Output

In my experience, it's really helpful to spend some time and plan out the final output of the data cleaning process.

1. How will the output(s) actually be used?
2. What does (1) say about what the output should look like?
3. How can you optimize (2) once you understand (1)?

In our case, the answer to (1) is that we will be plugging transition probabilities for a given team into a simulation function. In fact, we will have 4 such transitions per team:

* A 51x51 "scores" transition matrix for offense
* A 51x51 "scores" transition matrix for defense
* An 8x8 "states" transition matrix for offense
* An 8x8 "states" transition matrix for defense

For more clarification on why it's the case that we need 4 matrices, see [here](/2020/11/snappa-markov-creation#turning-points-into-wins-theoretically-and-practically)
Each of these matrices can be relatively time-consuming to calculate, so we don't want the application finding each of these matrices every time that a game starts. Instead, the app imports a list with each matrix **for every team that the database has ever observed**. Doing it in this way significantly alleviates problems related to putting a somewhat code-intensive cleaning process in the middle of a `Shiny` application^[This is particularly true because we're using the free version of [shinyapps](https://www.shinyapps.io/) to run our application, so ensuring a decent UX often means getting things to run as quickly as we can.]. 

My way of working within this framework was to aim for a named list that I call 
`transitions_list`. Each element of the list is indexed by a unique name for the 
team which is based off of the `serial_number` of all the team members. Within 
each list element, there are two other lists - one for "scores" and one for "states"- 
and each of these lists has one matrix for "offense" and one for "defense". For example,
if players with the `serial_number`s 2 and 9 are the only two players on the same team, 
the application would look for `transitions_list[["(2, 9)"]]`, but if the team
had players 2, 3, and 9 it would look for `transitions_list[["(2, 3, 9)"]]`.
Once the app is looking at the correct `TEAM_NAME`, it retrieves

* `transitions_list[[TEAM_NAME]]$scores$offense`
* `transitions_list[[TEAM_NAME]]$scores$defense`
* `transitions_list[[TEAM_NAME]]$states$offense`
* `transitions_list[[TEAM_NAME]]$states$defense`

Hence, the functions that I write to handle data manipulation are all done with 
the aim of obtaining outputs that follow this format.

## Preliminary Tables

I will eventually obtain a list of transition probabilities for all teams by using 
the `pmap` function in `purrr`. First, I create  a table which contains all teams
and the games in which they've played by using `pivot_wider` along with `performance`


```{r unique teams, echo = T, message= F}
library(tidyverse)
team_combos = performance %>% 
    group_by(game_number, side) %>%
    arrange(serial_number) %>%
    mutate(position = row_number()) %>%
    pivot_wider(id_cols = c(game_number, side), names_from = position, 
                values_from = serial_number)

# For demonstration
team_combos %>%
  arrange(desc(game_number)) %>% 
  head(15) %>%
  ungroup() %>%
  gt() %>%
    tab_header(title = "Team Combinations by Game Number and Side")
```
Notice how the players' `serial_number` translates into a way of uniquely identifying each team. One of the central tenets of the modeling that I'm performing is that I have no way of estimating how important each individual teammate is. I only understand teams by understanding **unique** combinations of players. Hence, each of these NA values are actually important because it is the sequence of `1`, `2`, `3`, and `4` which uniquely determine a team. For that reason, all functions that handle this data expect 4 values for a team, with `NA` being the default for the last two. However, we make these explicit `NA` values implicit when it comes time to create the vector which retrieves the proper team from `transitions_list`. 


For each team, we need to obtain a list of games in which we have seen that unique combination of players. I achieve that by creating a `filter` expression for all four numbered columns on `team_combos` as follows. Suppose we're again looking at team "(2, 9)" from before. This team would correspond to `1` = 2, `2` = 9, `3` = `NA`,
`4` = `NA`^[Note that we have to specify all of these for the reasons mentioned before: each team pairing refers to the unique combination of players, meaning (2,9) is different from (2,3,9)]. Also the use of column names whose values are numbers is really not great practice, but it does have the upshot of allowing me to write the `filter_expression` using `imap`. 
```{r filter expression, echo = T, message = F}
team_vector = c(2, 9, NA, NA)
filter_statements = team_vector %>%
  imap(function(player, index){
    if(!is.na(player)){
      str_c("`", index, "` == ", player)
    } else { 
      str_c("is.na(`", index, "`) ")
      }
  })
filter_expression = 
  rlang::parse_exprs(
    paste0(filter_statements)
)

game_side_pair = team_combos %>%
  filter(!!!filter_expression) %>%
  select(game_number, side)
```
`game_side_pair` tells us what games our chosen team is in, and also what side they were in that game. This may not seem too important, but it actually matters quite a bit. Ultimately we want to make sure that we are correctly identifying how many shots a team had in each game (read: how many opportunities they had to score), which forms the basis of the denominator of the maximum likelihood estimator of transition probabilities. For example, in games that team B wins, `NUMBER A` is the final round, which would mean that A got `NUMBER` shots on offense,
while B got `NUMBER - 1`! That same dynamic holds in reverse for defense. 

In moving towards that final total shots number, I first figure out how many shots are in each round. When games are unbalanced, like a 2 vs 3 scenario, the team with 2 people still shoots 3 times. This means that we have to know the number of players on the opposing team to also know the number of shots which a team had in a game. Each team's shots is equal to the maximum team size in that game. 

``` {r game shots, echo = T, message = F}
game_shots_pair = performance %>%
  filter(game_number %in% game_side_pair$game_number) %>%
  group_by(game_number, side) %>%
  summarize(team_size = n(), .groups = "drop_last") %>%
  summarize(shots = max(team_size), .groups = "drop_last")
```

Next is to obtain the number of rounds that each team played in each game. In `summary`, there is a variable called `total_rounds` which tracks the total number of rounds in a game (e.g. if the game had ended on round 1A, this value would be 1, and if the game ended on 12B, this would be 24). One can verify that the following `mutate` should provide answers to the side-specific question of the number of rounds taken.

``` {r make total_rounds, echo = T}
total_rounds = game_side_pair %>%
  left_join(summary %>%
              select(game_number, total_rounds),
            by = "game_number") %>%
  mutate(offense_rounds = ifelse(side == "A", ceiling(total_rounds / 2), floor(total_rounds / 2)),
         defense_rounds = ifelse(side == "A", floor(total_rounds / 2), ceiling(total_rounds / 2))) %>%
  ungroup() %>%
  select(game_number, offense_rounds, defense_rounds)

total_rounds
```
Finally, I also create a version of the `events` table which cuts away the clutter that we don't need, and provides the manipulations that would be needed for all the calculations. This is the object from which we will be most directly obtaining the counts
```{r simplified events, echo = T, warning= F}
events_simplified = events %>% 
    arrange(game_number, shot_number) %>%
    group_by(game_number, round, scored) %>%
    mutate(shot_order = row_number()) %>% 
    select(game_number, scored, total, round, shot_order) %>%
    filter(game_number %in% game_side_pair$game_number) %>%
    left_join(game_side_pair, by = "game_number") %>%
    filter(scored == side) %>%
    mutate(off_def = case_when(str_extract(round, "[A-Z]") == side ~ "offense",
                            str_extract(round, "[A-Z]") != side ~ "defense"),
           round_num = as.integer(str_sub(round, 1, -2))
           )
```
## Estimating the Probabilities - Valid Constructs  
With these tables in place, we're ready to begin forming the data set used to make transition probabilities. First, an overview on what that data set should look like. Since transition matrices are all about recording the number of times that a transition occurs, we will be using a vector of "running scores", i.e., the cumulative sum of points for a given team in a game. Those cumulative points should be calculated over a data set which has been properly ordered between offense and defense *for that team*. For example, if a team was "A" in a game, then their order should be offense -> defense. If a team was "B", it should be reversed.

Having said this, we arrive at the largest threat to validity for this entire method: the order in which shots are observed during a round. The scoreboard does not require players to record non-scoring events. Intuitively, this choice is made to improve the rate of play and minimize UI fatigue. In practice, however, it means that I have no way of systematically knowing whether the first point scored in a round was on the first shot. In fact, the only cases where I can be sure that the ordering is correct are cases where everyone on the team scores on that round. For all the rest, I am left to do the best I can. It's worth thinking about the effect that this has on the model. At the beginning of the round, let team A have two shots and `a` points, and suppose that one point is scored during the round. If that point was scored on the first shot, then the transitions matrix should record, in order:

1. A transition from `a` to `a + 1`
2.  A transition from `a + 1` to `a + 1`

In fact, this is what the transitions matrix would report too. Having seen one entry in `events` for this round, the script will give that scoring event the first position, and fill in the "implicit" 0 at the second. This becomes a threat to validity if the point was scored with the second shooter, since in that case the progression **should** be:

1. A transition from `a` to `a`
2. A transition from `a` to `a + 1`

However, the script would still perform the first set of steps, not the second, because it does not know to put the scoring event in the second position. This has the effect of "shifting" counts up and down. Notice through this example that both sets of steps include a transition from `a` to `a +  1`, but they put the "stagnant" transition in the wrong row and wrong column. This means that these counts are being shifted from one multinomial distribution to another. 

There is no way, given our current desire to limit the number of times that someone has to walk over to the app, for us to fix this problem with certainty, but I think there are quite plausible reasons to believe that this error does not completely negate my methodology:

1. Any non-zero score `a` can be a "donor" or a recipient, which implies that there is no reason to believe that this biases our results for those probability estimates, though it should certainly increase our uncertainty about the point estimates. This assumption becomes more tenuous to maintain as scores begin to fall outside of the conventional range, however, since we have a paucity of those events and each shift matters more as a result.
2. Because our real object of interest in this method is win probabilities, we can live with a little more uncertainty around the probability estimates. Whether a team's simulated score progression sees them stalling out from 13 to 13 or 14 to 14 is of minor concern to us. 
<!-- This isn't correct. Think about this more. The upshot of randomization is not that it improves your
chances of getting any one particular value correct. It's that it reduces the variance of your error term.
3. There may be ways to get around this issue, or to introduce some additional constructs to alleviate the issue. For example, if I were to instead randomly assign a position to each scoring event, that may have the effect of lowering the uncertainty introduced. In 2v2 games, I'd anticipate this would have no effect because my naive estimate of the chance of being correct is 50-50 regardless. In games with three players, however, this would improve the chance that I'm right about a round where one point was scored from $\frac{1}{3}$ to $\frac{1}{2}$.
-->
Having recognized and discussed this issue, we can now move on to the construction of the transitions matrix. 

## Constructing the Matrix
The matrix is constructed game-by-game: I produce two outputs whose values are the counts of the transitions in each game for each half (offense or defense). The matrices for each game are added with the matrices for all other games, and then the row totals are used to divide the elements of the row. That is, at row `r`, the total represents the number of times that a team has been observed at score or state `r-1`^[This is a technical consideration which is true because we need a row and column for zero points, and we cannot have a 0th row from R's perspective]. For a given `game`, then, I run the following:
```{r game assignment, include = F}

game = 93

```
```{r transitions map single game, echo = T, message= F}
rounds_in_game = total_rounds %>%
      filter(game_number == game) %>%
      select(offense_rounds, defense_rounds)
    
shots_per_round = game_shots_pair %>%
      filter(game_number == game) %>%
      pull(shots)
  
filtered_game = events_simplified %>%
      filter(game_number == game)
    
    # Keeping these two together before completing
    # proved to be too complicated for the factors
    # I needed to specify per side. The next steps 
    # make that
    # easier, but are admittedly kind of ugly
    
offense = filtered_game %>% 
      filter(off_def == "offense") %>%
      ungroup() %>%
      select(round_num, shot_order, off_def, total)
defense = filtered_game %>%
      filter(off_def == "defense") %>%
      ungroup() %>%
      select(round_num, shot_order, off_def, total)
    
offense_rounds = rounds_in_game$offense_rounds
defense_rounds = rounds_in_game$defense_rounds
    
offense$round_num = factor(
        offense$round_num,
        levels = seq(1, offense_rounds))
offense$shot_order =
      factor(
        offense$shot_order,
        levels = seq(1, shots_per_round))  
    
defense$round_num =
      factor(
        defense$round_num,
        levels = seq(1, defense_rounds))
defense$shot_order =
      factor(
        defense$shot_order,
        levels = seq(1, shots_per_round)) 


expanded_offense = offense %>%
      complete(
        expand(offense,
               round_num,
               shot_order),
        fill = list(total = 0,
                    off_def = "offense"))

expanded_defense = defense %>%
      complete(
        expand(defense,
               round_num,
               shot_order),
        fill = list(total = 0,
                    off_def = "defense"))

expanded_game = bind_rows(expanded_offense,
                              expanded_defense)

expanded_game
```

The table `expanded_game` has all of the pertinent information for a team's performance
in a given game. However, it would be incorrect to calculate transition probabilities at this point 
because the data set is unordered. We can recover ordering by remembering that the switch between
offense and defense in a round is predictable once we know the team's side.

```{r the rest of the owl, echo = T}
team = game_side_pair %>%
  filter(game_number == game) %>%
  pull(side)

if (team == "A"){
  expanded_game = expanded_game %>% 
        arrange(round_num, desc(off_def), shot_order) %>%
        select(off_def, total)
  
  expanded_game = bind_rows(tibble(off_def = "offense", total = 0),
                            expanded_game)
} else {
  expanded_game = expanded_game %>% 
        arrange(round_num, off_def, shot_order) %>%
        select(off_def, total)
  
  expanded_game = bind_rows(tibble(off_def = "defense", total = 0),
                            expanded_game)
}

score_side_table = expanded_game %>%
                        summarize(running_score = cumsum(total),
                                  off_def = off_def,
                                  .groups = "drop")
```
In each case, notice that I bind a new row with a score of 0 at the very beginning of the expanded
game. This is done to set the initial condition. If a team scores in round 1A on their very first shot
(or paddle), then that should be recorded as a transition from 0 to 1. That line ensures this is the case.

Finally, we take this object and simplify it into a table with two columns called `score_side_table`. This is the "clean" version of our data. From here, calculating transition counts is straightforward. Here, I'll demonstrate in the case that we're looking at "scores". 

```{r transition counts, echo = T}
offense_matrix = matrix(data = 0, nrow = 51, ncol = 51)

defense_matrix = matrix(data = 0, nrow = 51, ncol = 51)

# Create a matrix that will be used for our transition counts
for (i in 2:length(score_side_table$running_score)){
  old_score = score_side_table$running_score[i - 1]
  new_score = score_side_table$running_score[i]
  
  if (score_side_table$off_def[i] == "offense"){
    offense_matrix[old_score + 1, new_score + 1] =
      offense_matrix[old_score+ 1, new_score + 1] + 1
  } else { 
    defense_matrix[old_score + 1, new_score + 1] = 
            defense_matrix[old_score + 1, new_score + 1] + 1
  }
}
``` 
The two matrices which are created, one for offense and one for defense, are then added together with the counts
for every game in which that team has played. In fact, I `map` the sequence of functions above over the games in which that team has played. This provides us with objects that I call the `transition_counts`. 

Next, we put it all together.
```{r put it all together, echo = T, eval = F}
row_totals = map_dbl(seq(1, matrix_rank), function(number) { 
                                 transition_counts_offense[number,] %>% sum()
                                 }
                       )

transition_probs_offense = transition_counts_offense / row_totals
```
Overall, we do these steps 4 times: one for each matrix that is needed in the simulation. The app stores these matrices for every unique team combination which we have seen in Snappa history, and runs simulations by drawing scores from these matrices.

## Conclusion

I hope that you have enjoyed reading this post. For the practitioner, I hope that it provides insight into some ways that you can accomplish data cleaning goals while creating statistical objects that could not be easily accomplished with a package. For the more skilled practitioner than me, I'd love to hear your feedback on this process.


This article is the latest in a series on [Snappa](https://github.com/mdewey131/Snappa-Scoreboard/wiki/An-Overview-of-SnappaMetrics#what-is-snappa).
For more articles, see [here](/categories/#posts-list-snappa-scoreboard)