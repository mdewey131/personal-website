---
author: "Michael Dewey"
slug: "intro-hypothesis-testing-1"
date: "2020-11-23"
output: 
  blogdown::html_page
tags:
  - Rstats
  - statistics
  - Hypothesis Testing
categories:
  - Lessons
title: "Introduction to Hypothesis Testing, Part 1"
runtime: shiny
---
This lesson is intended to bring together many disparate but crucial elements
of statistics to explain how and why hypothesis testing works. Part 1 deals with
the mechanics of hypothesis testing, the types of hypotheses we formulate, and
some understanding of what can happen when we test. Part 2 will deal with the 
specific forms of tests that we make. The full version is intended for students 
in their first undergraduate statistics course and 
takes around half an hour to deliver in tutoring. However, more of the details
can easily be asserted for students in less rigorous courses, allowing this 
lecture to be delivered to high school students.

### Why Do We Test Hypotheses?
Throughout your statistics education, you have doubtless heard much about the 
**parameters** of a probability distribution within a population. In the view of
this lesson^[This refers to the frequentist view as opposed to a Bayesian one], 
the population's parameters are constant values which are *unknown* to us. To set 
an example which will persist throughout this lesson, consider the normal distribution.
The normal distribution is referred to in common parlance as the "bell curve"; we 
denote a population which has that distribution by writing

$$ X_i \stackrel{\text{iid}}{\text{~}} \mathcal{N}(\mu, \sigma^2) $$
The phrase "iid" means **independent and identically distributed**. In short, 
everyone in this population follows the same distribution, and all of the observations
are independent from one another. The assumption of independence is quite important
for some results that follow, so it's necessary to keep it in mind. The parameters
$\mu$ and $\sigma^2$ are the mean and the variance of the normal distribution^[
For the sake of generality, it's worth noting that that is a particular property
of the normal distribution. Other distributions, like the binomial, have parameters
which are not the distribution's mean or variance.]

If we knew the value of the parameters $\mu$ and $\sigma^2$, then we would fully 
know the distribution, meaning that we would perfectly know the probability
of having any value from that distribution. To see this in action, you can play
around with the normal distribution below. Click and drag along the graph to see
the chance of falling within the area:

<iframe height="600" width="100%" frameborder="no" src="https://michael-dewey.shinyapps.io/companion_app/?normal_distribution"> </iframe>


In truth, we will never perfectly know the values of the population, although we 
will assume that we know the variance at times. Hypothesis testing, then, is an
exercise in understanding what conclusions we can make about values we will never
know only by using values that we *do* know.

### A Preliminary Example: Repeated Coin Tosses
To wrap your head around how hypothesis testing works on a mechanical level, consider
the following example. I hand you a coin with two sides, heads and tails, and I 
tell you that the coin is "fair", meaning that the chances of getting either heads
or tails is 50%. Maybe out of a sense of boredom, or because you don't trust my
assertion^[Statistics is designed for uncertainty, so no offense taken at your curiousity.], 
you flip the coin. After ten flips, you find that six of them have come up with heads. 
After seeing those six heads, would you feel that you have enough evidence to reject
my claim that the coin is fair?

You probably shouldn't. It's not that unusual to see six heads out of ten when the
odds are 50-50. In fact, if you were to keep flipping ten times and recording the 
results, around 50% of all of those tests would show a number of heads between
four and six. If you decided to say something along the lines of "I will doubt that
this coin is fair if I see six heads or more" and I **wasn't** lying, there would
still be about a 26% chance that you erroneously doubt me. Which would hurt my
feelings. 

What if, instead, you flipped the coin 100 times, finding that 60 out of those
100 flips came up heads. Would you doubt me then? 

You might, and you'd be on stronger footing to make such a claim. If you kept
repeating this experiment, you'd find that the chance of getting something higher
than 60 heads is only about 2%. If you decided to say something along the lines of
"I will doubt that this coin is fair if I see sixty heads or more" and I wasn't
lying about the coin being 50-50, there would only be a 2% chance that you come
to the wrong conclusion. That's definitely better than 26%!

The important thing to keep in mind, however, is that you will **never** be able
to make a test which is both useful and never makes mistakes of this kind. Hypothesis 
testing asks us to make conclusions based on the data we gathered, which is not the same
thing as seeing the population parameter for ourselves. You will sometimes 
erroneously doubt me. That's okay. The important thing is designing a statement, 
like the ones above, which lower the chance of such a mistake to an acceptable level,
whatever that means for your problem. 

### Formulating Hypotheses
For the sake of illustration, I'll be using the same sets of hypotheses throughout
this page. All throughout, I will make it clear when what I'm saying is not 
general to all hypothesis testing but rather is specific to the hypotheses we
have chosen. We will also focus, for the sake of simplicity, on the case of single
hypothesis testing on the mean of a normal distribution. First, some definitions:
<ul>
  <li> The **null hypothesis**, $H_0$ is a statement of what we believe to be true in 
  the status quo. It is the best guess for a parameter value given common knowledge
  or past information. It is a statement of equality, like $H_0: \mu = 17$
  <li> The **alternative hypothesis** is a statement of what we believe to be
  true if the null hypothesis is not the case. 
  <li> An alternative hypothesis is **simple** if it is a statement of equality,
  such as $H_1: \mu = 20$. For reasons that are not covered here^[The curious reader
  may consult the [Neyman-Pearson Lemma](https://en.wikipedia.org/wiki/Neyman%E2%80%93Pearson_lemma)],
  we will ignore these hypotheses because their associated test is the same as those for
  one-sided alternatives.
  <li> An alternative hypothesis is **one-sided** if it is an inequality statement
  which is exclusively greater or exclusively less than the null hypothesis value.
  For example, a one-sided alternative hypothesis for the null above might be
  $H_1: \mu > 17$, while $H_1: \mu > 19$ would not be. 
  <li> An alternative hypothesis is **two-sided** if it is an inequality on both
  sides. For the null above, this would be stated as $H_1: \mu \not = 17$
</ul>

As mentioned above, there is no point in using a simple alternative because you
can do just as well with a one-sided alternative which is on the same side of
the mean as the simple value. There's no benefit to being particular in that
sense. Notice that I did not make such a claim when talking about the two-sided
alternative as compared to the one-sided, even though a two-sided test would be
**both** of the one-sided tests at the same time. Take a moment to think about
why this might be the case. That is, why might it true that a two-sided and
one-sided alternative hypothesis each have their own uses? We will come to see
the answer by the end of this post.

Before we move on to understand how we formulate tests in the next lesson, there are two 
important notes that close this portion.

##### Note 1: We Do Not "Accept The Null"
This point is repeated by every textbook covering hypothesis testing, and for 
good reason. Consider the example above about flipping coins. If you were to 
flip heads 55 times out of 100, you would not have good reason to believe that
the true odds **aren't** 50-50, but you don't necessarily have good evidence that 
it **is**, either. Flipping 55 heads out of 100 is also consistent with a slightly
unfair coin, say 55-45 or 60-40. It would be incorrect, then, to say that this
evidence proves that the true odds are 50-50. Instead, it only shows that you don't
have the evidence to prove it isn't 50-50, and hence we say that we "fail to reject
the null". 

##### Note 2: Tests are Fallible
As I've mentioned before, it's impossible to design a useful test which doesn't
make errors. Let's dig down to precisely understand the nature of these mistakes. 
Without getting too deep into the form of a specific test, let's say that there
are two conclusions one can reach: reject the null or do not reject the null. 
Moreover and simplifying heavily, there are two different things which can be 
"true" but unknown to us: either the null or the alternative hypothesis. Therefore,
we have a natural 2x2 grid whose four conclusions are represented below by memes. 

|  |$H_0$ | $H_1$|
|:---:|:-----:|:------:|
| Do Not Reject $H_0$ |![](/img/heck_yeah.jpg) | ![](/img/SadKeanu.jpg)  | 
|   Reject $H_0$      | ![](/img/alsosad.jpg)  | ![](/img/heck_yeah.jpg) | 

Obviously, we don't actually know what's true in reality. We only see what our tests show, so we can't truly know if we're supposed to be happy or not when we get our result. Two of these outcomes are bad for us, but one is worse than the other:

* When we reject the null hypothesis even though it was true in reality  (smoking Ben Affleck), we have made a *Type I Error*
* When we fail to reject a null and the alternative was true (sad Keanu), we have made a *Type II Error* 

Of these two, type I is considered worse. Consider the example of a court case. When we state hypotheses, we are putting our null hypothesis "on trial", and it can either be innocent or guilty, meaning untrue. Our test is the trial, where the evidence from a sample is taken into consideration and we make a verdict- either the null is guilty or it's "not guilty"^[notice we make no conclusion of innocence in an absolute sense off of the basis of our trial.]. Just like in a court case, it is considered far worse to convict an innocent null hypothesis than it is to let a guilty null go free. 
Without going too far into the details, we can quantify the probability of making either of these errors once we're given a test. We call the probability of type I error "alpha", that is $P(\text{Type I Error}) = P(\text{Reject a True Null}) = \alpha$. The probability of type II error is fittingly called "beta", that is $P(\text{Type II Error}) = P(\text{Fail to Reject a False Null}) = \beta$.

Those of you with some background in hypothesis testing already will no doubt notice that we already use alpha as a symbol for the significance level. This is not an accident: **a test at significance level alpha has at most "alpha" probability of making a type I error**. 

In other words, we define our tests based on our tolerance for convicting an innocent null.

To return to the coin flipping example, since there is only a 2% chance of doubting me 
when I was really correct (that is, a 2% chance of making a type I error when your alternative
hypothesis is that heads is more frequent), then your test "I will doubt you whenever
I see 60 heads or more" would be **significant** at the 2% level. Such a test would
also be allowable if you only wanted a 5% significance level, but you could actually
be less strict and still reject my claim at 5%. We will revisit this concept in a subsequent
entry on hypothesis testing and p-values. 


